{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370fd1d-6649-464f-a53d-06a86dceedca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96255fc2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd659a",
   "metadata": {},
   "source": [
    "#### Check missing column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def load_data():\n",
    "    \"\"\"load data.\"\"\"\n",
    "    f = open(f\"dataset/x_train.csv\")\n",
    "    features = f.readline()\n",
    "    feature_names = features.split(',')\n",
    "    data = np.loadtxt(f\"dataset/x_train.csv\", delimiter=\",\", skiprows=1, dtype=str)\n",
    "    return data,feature_names\n",
    "\n",
    "def convert_row_to_float(row):\n",
    "    \"\"\"Convert values in row to float or np.nan.\"\"\"\n",
    "    new_row = []\n",
    "    for item in row:\n",
    "        try:\n",
    "            new_row.append(float(item))\n",
    "        except ValueError:\n",
    "            new_row.append(np.nan)\n",
    "    return np.array(new_row)\n",
    "\n",
    "def convert_all_rows(data):\n",
    "    \"\"\"Convert all rows to float or np.nan.\"\"\"\n",
    "    new_data = []\n",
    "    for row in data:\n",
    "        new_data.append(convert_row_to_float(row))\n",
    "    return np.array(new_data)\n",
    "\n",
    "def column_NAN(array):\n",
    "    nan=0\n",
    "    for i in range(len(array)):\n",
    "        if np.isnan(array[i]):\n",
    "                nan += 1\n",
    "    return nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(data, ratio, seed):\n",
    "    \"\"\"Split data into training and validation set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(data)\n",
    "    split_index = int(len(data) * ratio)\n",
    "    return data[:split_index], data[split_index:]\n",
    "\n",
    "def k_fold_split(data, k, seed):\n",
    "    \"\"\"Split data into k folds.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(data)\n",
    "    return np.array_split(data, k)\n",
    "\n",
    "def standardize_data(data):\n",
    "    \"\"\"Standardize data.\"\"\"\n",
    "    mean = np.nanmean(data, axis=0)\n",
    "    std = np.nanstd(data, axis=0)\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36110a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data, features = load_data()\n",
    "import pandas as pd\n",
    "pandas_df = pd.read_csv(\"dataset/x_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data, features = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6803ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pandas_df)\n",
    "print(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = numpy_data[0]\n",
    "new_first_row = convert_row_to_float(first_row)\n",
    "# Compare the values of the firs row in the numpy array and the pandas dataframe\n",
    "for i, j in zip(pandas_df.iloc[0,:].values, new_first_row):\n",
    "    print(f\"Pandas value: {i} | Numpy value: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_all_rows(numpy_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45510a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaNs in each column\n",
    "number_of_nans = pandas_df.isnull().sum()\n",
    "# Sort in ascending order\n",
    "number_of_nans.sort_values(inplace=True, ascending=False)\n",
    "for index, value in zip(number_of_nans.index, number_of_nans.values):\n",
    "    print(f\"Column {index} has {value} NaNs\")\n",
    "# This shows that for some columns we essentially have no meaningful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in features:\n",
    "    print(f\"feature {i} has {column_NAN(data[:,features.index(i)])} NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40712763",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_NAN(data[:,features.index('COLGHOUS')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of observations: {len(pandas_df)}\")\n",
    "print(f\" 10% of the number of observations: {round(len(pandas_df) * 0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c695a5",
   "metadata": {},
   "source": [
    "Here we can decide which columns to drop based on the number of nans in the <span style=\"color:red\"> test set </span> . \\\n",
    "Since at the end of the day we want to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns with less than 10% NaNs\n",
    "columns_to_keep = []\n",
    "for index, value in zip(number_of_nans.index, number_of_nans.values):\n",
    "    if value < round(len(pandas_df) * 0.1):\n",
    "        columns_to_keep.append(index)\n",
    "# Keep only the columns with less than 10% NaNs\n",
    "data_with_few_nans = pandas_df.loc[:, columns_to_keep]\n",
    "\n",
    "# Save the columns_to_keep list to a pickle file\n",
    "with open(\"columns_to_keep_at_90.pkl\", \"wb\") as f:\n",
    "    pickle.dump(columns_to_keep, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ea7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df.shape)\n",
    "print(data_with_few_nans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cound the number of unique values in each column\n",
    "number_of_unique_values = data_with_few_nans.nunique()\n",
    "# Sort in ascending order\n",
    "number_of_unique_values.sort_values(inplace=True, ascending=False)\n",
    "for index, value in zip(number_of_unique_values.index, number_of_unique_values.values):\n",
    "    print(f\"Column {index} has {value} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dcfe1",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Here we also need to make a decision what threshold to use for columns to drop. </span> \n",
    "\n",
    "Could be that we use:\n",
    "- Even higher threshold than 90%\n",
    "- A smaller threshold and try to interpolate the data from other features.\n",
    "\n",
    "Optimally: I think what we want is to <span style=\"color:red\"> have train -test sets independent of what columns we dropped </span> that way we could compare the loss across models which use different numbers of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74268e3",
   "metadata": {},
   "source": [
    "## a) Carry on Datanalysis only with columns having > 90% of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values for each row\n",
    "number_of_nans_per_row = data_with_few_nans.isnull().sum(axis=1)\n",
    "# Sort in ascending order\n",
    "number_of_nans_per_row.sort_values(inplace=True, ascending=False)\n",
    "for index, value in zip(number_of_nans_per_row.index, number_of_nans_per_row.values):\n",
    "    print(f\"Row {index} has {value} NaNs\")\n",
    "\n",
    "# Plot the histogram of the number of missing values per row\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(number_of_nans_per_row.values, bins=20)\n",
    "plt.xlabel(\"Number of NaNs\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of NaNs per row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0de07",
   "metadata": {},
   "source": [
    "We see that for most rows we actually have around 0 Missing values, which is pretty good for the Test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68aa59b",
   "metadata": {},
   "source": [
    "We have a number of choices here: \n",
    "- Drop rows which have say more than 10 missing value.\n",
    "- Replace all the Nan values with zeros.\n",
    "- Replace all the Nan values with medians, means, modes\n",
    "- Use some sort of interpolation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e555ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the y_train data\n",
    "y_train = pd.read_csv(\"dataset/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906eddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove the columns with more than 10% of Nans using  the columns_to_keep list\n",
    "x_train_reduced_90 = data_with_few_nans\n",
    "\n",
    "# Again count the number of NaNs in each column\n",
    "number_of_nans = x_train_reduced_90.isnull().sum()\n",
    "number_of_nans.sort_values(inplace=True, ascending=False)\n",
    "for index, value in zip(number_of_nans.index, number_of_nans.values):\n",
    "    print(f\"Column {index} has {value} NaNs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9012f81",
   "metadata": {},
   "source": [
    "Looks quite similar to the Test data fortunately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec99685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_by_missing_values(data, threshold):\n",
    "    \"\"\"Drop rows with more than threshold missing values.\"\"\"\n",
    "    number_of_nans_per_row = data.isnull().sum(axis=1)\n",
    "    rows_to_drop = number_of_nans_per_row[number_of_nans_per_row > threshold].index\n",
    "    new_data = data.drop(rows_to_drop)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd32c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_values_with_mean(data):\n",
    "    \"\"\"Replace missing values with the mean of the column.\"\"\"\n",
    "    new_data = data.fillna(data.mean())\n",
    "    return new_data\n",
    "\n",
    "def replace_missing_values_with_median(data):\n",
    "    \"\"\"Replace missing values with the median of the column.\"\"\"\n",
    "    new_data = data.fillna(data.median())\n",
    "    return new_data\n",
    "\n",
    "def replace_missing_values_with_mode(data):\n",
    "    \"\"\"Replace missing values with the mode of the column.\"\"\"\n",
    "    new_data = data.fillna(data.mode().iloc[0])\n",
    "    return new_data\n",
    "\n",
    "def replace_missing_values_with_zero(data):\n",
    "    \"\"\"Replace missing values with zero.\"\"\"\n",
    "    new_data = data.fillna(0)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d29a41",
   "metadata": {},
   "source": [
    "#### Scenario 1)\n",
    "#### Do not Drop data, replace the missing values in each row with column means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fa8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "df_train_reduced_90_mean = standardize_data(x_train_reduced_90)\n",
    "#Fill in the missing values with the mean of the column\n",
    "df_train_reduced_90_mean = replace_missing_values_with_mean(df_train_reduced_90_mean)\n",
    "# Split the data into training and validation set\n",
    "train_data, validation_data = train_validation_split(df_train_reduced_90_mean.values, 0.8, 42)\n",
    "print(train_data.shape)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112b8e8",
   "metadata": {},
   "source": [
    "Possibly also add a bias term to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b737022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights using the normal distribution\n",
    "np.random.seed(42)\n",
    "initial_weights = np.random.normal(size=train_data.shape[1])\n",
    "# Fit the model using Linear Regression with Gradient Descent\n",
    "w_ols_sgd, loss = mean_squared_error_gd(y_train.values, train_data, initial_w=initial_weights, max_iters=1000, gamma=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
