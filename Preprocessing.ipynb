{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb178f83",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ac39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def load_data():\n",
    "    \"\"\"load data.\"\"\"\n",
    "    f = open(f\"dataset/x_train.csv\")\n",
    "    features = f.readline()\n",
    "    feature_names = features.split(',')\n",
    "    data = np.loadtxt(f\"dataset/x_train.csv\", delimiter=\",\", skiprows=1, dtype=str)\n",
    "    return data,feature_names\n",
    "\n",
    "def convert_row_to_float(row):\n",
    "    \"\"\"Convert values in row to float or np.nan.\"\"\"\n",
    "    new_row = []\n",
    "    for item in row:\n",
    "        try:\n",
    "            new_row.append(float(item))\n",
    "        except ValueError:\n",
    "            new_row.append(np.nan)\n",
    "    return np.array(new_row)\n",
    "\n",
    "def convert_all_rows(data):\n",
    "    \"\"\"Convert all rows to float or np.nan.\"\"\"\n",
    "    new_data = []\n",
    "    for row in data:\n",
    "        new_data.append(convert_row_to_float(row))\n",
    "    return np.array(new_data)\n",
    "\n",
    "def column_NAN(array):\n",
    "    nan=0\n",
    "    for i in range(len(array)):\n",
    "        if np.isnan(array[i]):\n",
    "                nan += 1\n",
    "    return nan\n",
    "\n",
    "def train_validation_split(data, ratio, seed):\n",
    "    \"\"\"Split data into training and validation set.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(data)\n",
    "    split_index = int(len(data) * ratio)\n",
    "    return data[:split_index], data[split_index:]\n",
    "\n",
    "def k_fold_split(data, k, seed):\n",
    "    \"\"\"Split data into k folds.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(data)\n",
    "    return np.array_split(data, k)\n",
    "\n",
    "def standardize_data(data):\n",
    "    \"\"\"Standardize data.\"\"\"\n",
    "    mean = np.nanmean(data, axis=0)\n",
    "    std = np.nanstd(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "def number_of_NaNs(list_,data):\n",
    "    NaNs=np.zeros(len(list_))\n",
    "    for i in list_:\n",
    "        NaNs[list_.index(i)]=column_NAN(data[:,list_.index(i)])\n",
    "    return NaNs\n",
    "\n",
    "def removing_features(number_NaN,list_,data):\n",
    "    Removed_features=[]\n",
    "    for i in range(len(NaNs)):\n",
    "        if NaNs[i] > round(len(data))*0.1:\n",
    "            Removed_features.append(i)\n",
    "    reduced_data = np.delete(data, Removed_features, 1)\n",
    "    reduced_list = list(filter(lambda x: list_.index(x) not in Removed_features, list_))\n",
    "    return reduced_data, reduced_list\n",
    "\n",
    "def clean_outliers(X):\n",
    "    for column in range(X.shape[1]):  # for each feature:\n",
    "        # calculating 25th and 75th quantiles\n",
    "        q1_X = np.nanquantile(X[:, column], 0.25, axis=0)\n",
    "        q3_X = np.nanquantile(X[:, column], 0.75, axis=0)\n",
    "        IQR_X = q3_X - q1_X  # inter quantile range\n",
    "        # calculating lower and upper bounds\n",
    "        lower_bound = q1_X - 1.5 * IQR_X\n",
    "        upper_bound = q3_X + 1.5 * IQR_X\n",
    "        # finding which observations are outside/inside of the bounds\n",
    "        above = X[:, column] > upper_bound\n",
    "        below = X[:, column] < lower_bound\n",
    "        outside = above | below\n",
    "        inside = np.invert(outside)\n",
    "        # calculate median value of observations that are inside boundaries\n",
    "        median = np.median(X[inside, column])\n",
    "        # setting outliers equal to median\n",
    "        X[outside, column] = median\n",
    "    return X\n",
    "\n",
    "def replace_NaN_mean_column(column):\n",
    "    mean=np.nanmean(column)\n",
    "    for i in range(len(column)):\n",
    "        if np.isnan(column[i]):\n",
    "            column[i]=mean\n",
    "    return column\n",
    "\n",
    "\n",
    "def replace_NaN_mean(matrix): \n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:,i] = replace_NaN_mean_column(matrix[:,i])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36db846",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data, features = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76aa35",
   "metadata": {},
   "source": [
    "#### Converting to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_all_rows(numpy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66dc7b",
   "metadata": {},
   "source": [
    "#### Removing id column in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.delete(data,0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ec284",
   "metadata": {},
   "source": [
    "#### Finding number of NaN for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb10c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NaNs = number_of_NaNs(features,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dde2f6",
   "metadata": {},
   "source": [
    "#### Histogram NaN before removing colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd31251",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(NaNs, bins=20)\n",
    "plt.xlabel(\"Number of NaNs\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of NaNs per feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e27a21",
   "metadata": {},
   "source": [
    "#### Removing features with too many NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data, reduced_features = removing_features(NaNs,features,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7508d",
   "metadata": {},
   "source": [
    "#### Histogram NaNs after dropping features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc98b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reduced_NaNs = number_of_NaNs(reduced_features,reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(Reduced_NaNs, bins=20)\n",
    "plt.xlabel(\"Number of NaNs\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of NaNs per feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f39bd",
   "metadata": {},
   "source": [
    "Still quite a few features with a lot of NaNs (more than 20000) might consider increasing min level to 95 or 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ed154",
   "metadata": {},
   "source": [
    "#### Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_outlier_data = clean_outliers(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d622a",
   "metadata": {},
   "source": [
    "#### Dealing with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5300b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data_mean = replace_NaN_mean(removed_outlier_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_test=number_of_NaNs(reduced_features,reduced_data_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(NaN_test, bins=20)\n",
    "plt.xlabel(\"Number of NaNs\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of NaNs per feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17c6aa",
   "metadata": {},
   "source": [
    "We can also try with mode or other stuffs, but here I computed the mean before the NaNs, not sure if it impacts a lot the results to do it the other way around"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0700c6",
   "metadata": {},
   "source": [
    "Now we need to deal on the categorical features, I don't know how you want to proceed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
