{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "import seaborn as sns\n",
    "from preprocess import *\n",
    "from helpers_given import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train loaded\n",
      "x_train loaded\n",
      "x_test loaded\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids =  load_csv_data('dataset', sub_sample=False)\n",
    "with open('dataset/x_train.csv', 'r') as f:\n",
    "    features_string = f.readline()\n",
    "    features = features_string.split(',')\n",
    "features = features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x_train\n",
    "\n",
    "# faster alternative for number of nans:\n",
    "NaNs = nb_of_nans(data)\n",
    "# convert y from {-1,1} to {0,1} to avoid problems with logistic\n",
    "y_01 = convert_minus1_to_0(y_train)\n",
    "\n",
    "# Get reduced data\n",
    "reduced_data, reduced_features, Removed_features = removing_features(NaNs,features,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished for feature: _STATE\n",
      " Finished for feature: FMONTH\n",
      " Finished for feature: IDATE\n",
      " Finished for feature: IMONTH\n",
      " Finished for feature: IDAY\n",
      " Finished for feature: IYEAR\n",
      " Finished for feature: DISPCODE\n",
      " Finished for feature: SEQNO\n",
      " Finished for feature: _PSU\n",
      " Finished for feature: GENHLTH\n",
      " Finished for feature: PHYSHLTH\n",
      " Finished for feature: MENTHLTH\n",
      " Finished for feature: HLTHPLN1\n",
      " Finished for feature: PERSDOC2\n",
      " Finished for feature: MEDCOST\n",
      " Finished for feature: CHECKUP1\n",
      " Finished for feature: BPHIGH4\n",
      " Finished for feature: BLOODCHO\n",
      " Finished for feature: CVDSTRK3\n",
      " Finished for feature: ASTHMA3\n",
      " Finished for feature: CHCSCNCR\n",
      " Finished for feature: CHCOCNCR\n",
      " Finished for feature: CHCCOPD1\n",
      " Finished for feature: HAVARTH3\n",
      " Finished for feature: ADDEPEV2\n",
      " Finished for feature: CHCKIDNY\n",
      " Finished for feature: DIABETE3\n",
      " Finished for feature: SEX\n",
      " Finished for feature: MARITAL\n",
      " Finished for feature: EDUCA\n",
      " Finished for feature: RENTHOM1\n",
      " Finished for feature: VETERAN3\n",
      " Finished for feature: EMPLOY1\n",
      " Finished for feature: CHILDREN\n",
      " Finished for feature: INCOME2\n",
      " Finished for feature: INTERNET\n",
      " Finished for feature: WEIGHT2\n",
      " Finished for feature: HEIGHT3\n",
      " Finished for feature: QLACTLM2\n",
      " Finished for feature: USEEQUIP\n",
      " Finished for feature: BLIND\n",
      " Finished for feature: DECIDE\n",
      " Finished for feature: DIFFWALK\n",
      " Finished for feature: DIFFDRES\n",
      " Finished for feature: DIFFALON\n",
      " Finished for feature: SMOKE100\n",
      " Finished for feature: USENOW3\n",
      " Finished for feature: ALCDAY5\n",
      " Finished for feature: FRUITJU1\n",
      " Finished for feature: FRUIT1\n",
      " Finished for feature: FVBEANS\n",
      " Finished for feature: FVGREEN\n",
      " Finished for feature: FVORANG\n",
      " Finished for feature: VEGETAB1\n",
      " Finished for feature: EXERANY2\n",
      " Finished for feature: STRENGTH\n",
      " Finished for feature: SEATBELT\n",
      " Finished for feature: FLUSHOT6\n",
      " Finished for feature: PNEUVAC3\n",
      " Finished for feature: HIVTST6\n",
      " Finished for feature: QSTVER\n",
      " Finished for feature: QSTLANG\n",
      " Finished for feature: _STSTR\n",
      " Finished for feature: _STRWT\n",
      " Finished for feature: _RAWRAKE\n",
      " Finished for feature: _WT2RAKE\n",
      " Finished for feature: _DUALUSE\n",
      " Finished for feature: _LLCPWT\n",
      " Finished for feature: _RFHLTH\n",
      " Finished for feature: _HCVU651\n",
      " Finished for feature: _RFHYPE5\n",
      " Finished for feature: _CHOLCHK\n",
      " Finished for feature: _LTASTH1\n",
      " Finished for feature: _CASTHM1\n",
      " Finished for feature: _ASTHMS1\n",
      " Finished for feature: _DRDXAR1\n",
      " Finished for feature: _PRACE1\n",
      " Finished for feature: _MRACE1\n",
      " Finished for feature: _HISPANC\n",
      " Finished for feature: _RACE\n",
      " Finished for feature: _RACEG21\n",
      " Finished for feature: _RACEGR3\n",
      " Finished for feature: _RACE_G1\n",
      " Finished for feature: _AGEG5YR\n",
      " Finished for feature: _AGE65YR\n",
      " Finished for feature: _AGE80\n",
      " Finished for feature: _AGE_G\n",
      " Finished for feature: HTIN4\n",
      " Finished for feature: HTM4\n",
      " Finished for feature: WTKG3\n",
      " Finished for feature: _BMI5\n",
      " Finished for feature: _BMI5CAT\n",
      " Finished for feature: _RFBMI5\n",
      " Finished for feature: _CHLDCNT\n",
      " Finished for feature: _EDUCAG\n",
      " Finished for feature: _INCOMG\n",
      " Finished for feature: _SMOKER3\n",
      " Finished for feature: _RFSMOK3\n",
      " Finished for feature: DRNKANY5\n",
      " Finished for feature: DROCDY3_\n",
      " Finished for feature: _RFBING5\n",
      " Finished for feature: _DRNKWEK\n",
      " Finished for feature: _RFDRHV5\n",
      " Finished for feature: FTJUDA1_\n",
      " Finished for feature: FRUTDA1_\n",
      " Finished for feature: BEANDAY_\n",
      " Finished for feature: GRENDAY_\n",
      " Finished for feature: ORNGDAY_\n",
      " Finished for feature: VEGEDA1_\n",
      " Finished for feature: _MISFRTN\n",
      " Finished for feature: _MISVEGN\n",
      " Finished for feature: _FRTRESP\n",
      " Finished for feature: _VEGRESP\n",
      " Finished for feature: _FRUTSUM\n",
      " Finished for feature: _FRTLT1\n",
      " Finished for feature: _VEGLT1\n",
      " Finished for feature: _FRT16\n",
      " Finished for feature: _VEG23\n",
      " Finished for feature: _FRUITEX\n",
      " Finished for feature: _VEGETEX\n",
      " Finished for feature: _TOTINDA\n",
      " Finished for feature: MAXVO2_\n",
      " Finished for feature: FC60_\n",
      " Finished for feature: STRFREQ_\n",
      " Finished for feature: PAMISS1_\n",
      " Finished for feature: _PACAT1\n",
      " Finished for feature: _PAINDX1\n",
      " Finished for feature: _PA150R2\n",
      " Finished for feature: _PA300R2\n",
      " Finished for feature: _PA30021\n",
      " Finished for feature: _PASTRNG\n",
      " Finished for feature: _PAREC1\n",
      " Finished for feature: _PASTAE1\n",
      " Finished for feature: _LMTACT1\n",
      " Finished for feature: _LMTWRK1\n",
      " Finished for feature: _LMTSCL1\n",
      " Finished for feature: _RFSEAT2\n",
      " Finished for feature: _RFSEAT3\n",
      " Finished for feature: _AIDTST3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove the same columns from x_test\n",
    "reduced_x_test = np.delete(x_test, Removed_features, 1)\n",
    "\n",
    "# Get feature correlation dictionary\n",
    "feature_correlation_dict = create_dictionary_from_correlation(reduced_data,reduced_features,0.6)\n",
    "\n",
    "max_corrr_feature_dict = {}\n",
    "\n",
    "# Find the 50 most correlated features\n",
    "for key, val in feature_correlation_dict.items():\n",
    "    max_corrr_feature_dict[key] = len(val)\n",
    "\n",
    "# Sort the dictionary by value in descending order\n",
    "max_corrr_feature_dict = {k: v for k, v in sorted(max_corrr_feature_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = []\n",
    "for key in max_corrr_feature_dict.keys():\n",
    "    #Add features that have at least one correlation with another feature\n",
    "    if max_corrr_feature_dict[key] > 0:\n",
    "        features_to_drop.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant features\n",
    "redundant_features = [ 'FMONTH','IDATE','IMONTH','IDAY','IYEAR', 'SEQNO', '_STATE', '_PSU', ]\n",
    "for feature in redundant_features:\n",
    "        features_to_drop.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = []\n",
    "for feature in reduced_features:\n",
    "    if feature not in features_to_drop:\n",
    "        features_to_keep.append(feature)\n",
    "        \n",
    "# Also replace some features with their calculated counterparts\n",
    "origin_calculated_features = {\n",
    "    'WEIGHT2' : 'WTKG3',\n",
    "    'HEIGHT3' : 'HTM4',\n",
    "    'ALCDAY5' : '_DRNKWEK',\n",
    "    'FRUITJU1' : 'FTJUDA1_',\n",
    "    'FRUIT1' : 'FRUTDA1_',\n",
    "    'FVBEANS' : 'BEANDAY_',\n",
    "    'FVGREEN' : 'GRENDAY_',\n",
    "    'FVORANG' : 'ORNGDAY_',\n",
    "    'VEGETAB1' : 'VEGEDA1_',\n",
    "    'STRENGTH' : 'STRFREQ_'\n",
    "}\n",
    "\n",
    "# In features_to_keep replace the key of origin_calculated_features with the value\n",
    "for key, val in origin_calculated_features.items():\n",
    "    for i, feature in enumerate(features_to_keep):\n",
    "        if key == feature:\n",
    "            features_to_keep[i] = val\n",
    "\n",
    "# Drop duplicates\n",
    "features_to_keep = list(set(features_to_keep))\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_features_indices = []\n",
    "for feature in features_to_keep:\n",
    "    selected_features_indices.append(reduced_features.index(feature))\n",
    "\n",
    "selected_features_indices = sorted(selected_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = reduced_data[:, selected_features_indices]\n",
    "reduced_x_test = reduced_x_test[:, selected_features_indices]\n",
    "\n",
    "# Also remove the features from the reduced_features list\n",
    "reduced_features_2 = []\n",
    "for feature in reduced_features:\n",
    "    if feature in features_to_keep:\n",
    "        reduced_features_2.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_features_threshold(number_NaN,list_,data,thresh):\n",
    "    Removed_features=[]\n",
    "    for i in range(len(number_NaN)):\n",
    "        if number_NaN[i] > round(len(data))*thresh:\n",
    "            Removed_features.append(i)\n",
    "    reduced_data = np.delete(data, Removed_features, 1)\n",
    "    reduced_list = list(filter(lambda x: list_.index(x) not in Removed_features, list_))\n",
    "    return reduced_data, reduced_list, Removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of nans for the reduced data\n",
    "NaNs_reduced = nb_of_nans(reduced_data)\n",
    "\n",
    "#Get the reduced data\n",
    "reduced_data_2, reduced_features_3, Removed_features_2 = removing_features_threshold(NaNs_reduced,reduced_features_2,reduced_data,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_x_test_2 = np.delete(reduced_x_test, Removed_features_2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nine values with NaNs\n",
    "replace_nine_with_nan(reduced_data_2)\n",
    "replace_nine_with_nan(reduced_x_test_2)\n",
    "\n",
    "# Replace 99 values with NaNs\n",
    "replace_99_with_nan(reduced_data_2)\n",
    "replace_99_with_nan(reduced_x_test_2)\n",
    "\n",
    "reduced_data_2 = clean_outliers_modified(reduced_data_2)\n",
    "# should we do this for x_test too ?\n",
    "\n",
    "# For the _DRNKWEK feature, replace 9990 with NaN\n",
    "# TODO: maybe we could vectorize this so we don't do a loop over the data\n",
    "for i in range(reduced_data_2.shape[0]): \n",
    "    if reduced_data_2[i, reduced_features_3.index('_DRNKWEK')] == 9990:\n",
    "        reduced_data_2[i, reduced_features_3.index('_DRNKWEK')] = np.nan\n",
    "for i in range(reduced_x_test.shape[0]): \n",
    "    if reduced_x_test_2[i, reduced_features_3.index('_DRNKWEK')] == 9990:\n",
    "        reduced_x_test_2[i, reduced_features_3.index('_DRNKWEK')] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_low_cardinality_features(data):\n",
    "    \"\"\"\n",
    "    One-hot encodes columns with less than or equal to 4 unique values in a NumPy array.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): The input data matrix.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The one-hot encoded data matrix.\n",
    "    \"\"\"\n",
    "    encoded_data = data.copy()\n",
    "    num_rows, num_cols = data.shape\n",
    "\n",
    "    num_of_new = 0\n",
    "\n",
    "    for col in range(num_cols):\n",
    "        unique_values = np.unique(data[:, col])\n",
    "        num_unique = len(unique_values)\n",
    "\n",
    "        if num_unique <= 4:\n",
    "            # Create a binary mask for each unique value\n",
    "            masks = [data[:, col] == val for val in unique_values]\n",
    "\n",
    "            \n",
    "\n",
    "            # Create one-hot encoded columns\n",
    "            one_hot_columns = np.vstack(masks).T.astype(int)\n",
    "            last_column_index = one_hot_columns.shape[1] - 1\n",
    "            one_hot_columns = np.delete(one_hot_columns, last_column_index, axis=1)\n",
    "\n",
    "            # Remove the original column and insert the one-hot columns\n",
    "            encoded_data = np.delete(encoded_data, col+num_of_new, axis=1)\n",
    "            for i in range(0, one_hot_columns.shape[1]):\n",
    "                encoded_data = np.insert(encoded_data, col+num_of_new, one_hot_columns[:,i], axis=1)\n",
    "\n",
    "            num_of_new = num_of_new + len(masks) - 2\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_reduced_2 = one_hot_encode_low_cardinality_features(reduced_data_2)\n",
    "one_hot_reduced_x_test_2 = one_hot_encode_low_cardinality_features(reduced_x_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with medians\n",
    "reduced_median = replace_NaN(one_hot_reduced_2, method='median')\n",
    "reduced_median_test = replace_NaN(one_hot_reduced_x_test_2, method='median')\n",
    "\n",
    "# Standardize the data\n",
    "std_x_med = standardize_data(reduced_median)\n",
    "std_test_med = standardize_data(reduced_median_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-fold CV fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "NB_COL = std_x_med.shape[1] # corresponds to 'D' = number of features\n",
    "NB_ROWS = std_test_med.shape[0] # corresponds to 'N' = number of observations/respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished for degree: 0\n",
      " Finished for degree: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeanb\\ML\\ML_Project_1\\helpers.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-z))\n",
      "C:\\Users\\jeanb\\ML\\ML_Project_1\\helpers.py:75: RuntimeWarning: overflow encountered in exp\n",
      "  loss = np.log(1 + np.exp(tx @ w)) - y * (tx @ w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished for degree: 2\n",
      " Finished for degree: 3\n",
      " Finished for degree: 4\n"
     ]
    }
   ],
   "source": [
    "# CV on two hyperparams degree and lambda on regularized logistic regression adding polynomial functi\n",
    "seed = 12\n",
    "degrees = np.arange(0,5,1)\n",
    "k_fold = 4\n",
    "lambdas = np.arange(0.0,0.5,0.1)\n",
    "gamma = 0.5\n",
    "max_iters = 50\n",
    "initial_w = np.zeros(NB_COL)\n",
    "k_indices = build_k_indices(y_01, k_fold, seed)\n",
    "#for each degree, we compute the best lambdas and the associated rmse\n",
    "best_lambdas = []\n",
    "best_F1s = []\n",
    "#vary degree\n",
    "for degree in degrees:\n",
    "    # cross validation\n",
    "    F1_te = []\n",
    "    for lambda_ in lambdas:\n",
    "        F1_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            _, F1_test = cross_validation(y_01, std_x_med,initial_w, max_iters, gamma, k_indices, k, lambda_, degree)\n",
    "            F1_te_tmp.append(F1_test)\n",
    "        F1_te.append(np.mean(F1_te_tmp))\n",
    "    print(f\" Finished for degree: {degree}\")\n",
    "    ind_lambda_opt = np.argmax(F1_te)\n",
    "    best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "    best_F1s.append(F1_te[ind_lambda_opt])\n",
    "ind_best =  np.nanargmax(best_F1s)      \n",
    "best_degree = degrees[ind_best]\n",
    "best_lambda = best_lambdas[ind_best]\n",
    "best_F1 = best_F1s[ind_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0.4\n",
      "0.381872358963031\n"
     ]
    }
   ],
   "source": [
    "print(ind_best)\n",
    "print(best_degree)\n",
    "print(best_lambda)\n",
    "print(best_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run regularized logistic regression with best param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = build_poly(std_x_med, best_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_w = tx.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23926742026026468\n",
      "[-1. -1. -1. ... -1. -1. -1.]\n",
      "0.9139622411507459\n"
     ]
    }
   ],
   "source": [
    "w_reg, loss_reg = reg_logistic_regression(y_01, tx, initial_w = np.zeros(size_w), max_iters = 50, gamma = 0.5, lambda_ = best_lambda\n",
    "print(loss_reg)\n",
    "y_reg = convert_0_to_minus1(convert_predict(tx @ w_reg))\n",
    "print(y_reg)\n",
    "# accuracy\n",
    "p_reg = compute_accuracy(y_train, y_reg) #percentage of false predictions\n",
    "print(p_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test = build_poly(std_test_med, best_degree)\n",
    "y_pred = convert_0_to_minus1(convert_predict(tx_test @ w_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, y_pred, \"new_result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
