{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_given import load_csv_data\n",
    "from preprocess import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data with helpers_given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train loaded\n",
      "x_train loaded\n",
      "x_test loaded\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"dataset/\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "328135\n"
     ]
    }
   ],
   "source": [
    "nb_col = x_train.shape[1]\n",
    "nb_rows = x_train.shape[0]\n",
    "print(nb_col)\n",
    "print(nb_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nans = np.zeros(nb_col)\n",
    "for i, col in enumerate(x_train.T):\n",
    "    nb_nans[i] = np.count_nonzero(np.isnan(col))\n",
    "\n",
    "reduced_data, reduced_features = removing_features(nb_nans, train_ids, x_train)\n",
    "\"\"\"feature_correlation_dict gives an index error\n",
    "\n",
    "# Get feature correlation dictionary\n",
    "feature_correlation_dict = create_dictionary_from_correlation(reduced_data,reduced_features,0.6)\n",
    "\n",
    "max_corrr_feature_dict = {}\n",
    "\n",
    "# Find the 50 most correlated features\n",
    "for key, val in feature_correlation_dict.items():\n",
    "    max_corrr_feature_dict[key] = len(val)\n",
    "\n",
    "# Sort the dictionary by value in descending order\n",
    "max_corrr_feature_dict = {k: v for k, v in sorted(max_corrr_feature_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "features_to_drop = []\n",
    "for key in max_corrr_feature_dict.keys():\n",
    "    features_to_drop.append(key)\n",
    "    if len(features_to_drop) == 30:\n",
    "        break\n",
    "\n",
    "# Define features to keep\n",
    "features_to_keep = []\n",
    "for feature in reduced_features:\n",
    "    if feature not in features_to_drop:\n",
    "        features_to_keep.append(feature)\n",
    "\n",
    "# Also replace some features with their calculated counterparts\n",
    "origin_calculated_features = {\n",
    "    'WEIGHT2' : 'WTKG3',\n",
    "    'HEIGHT3' : 'HTM4',\n",
    "    'ALCDAY5' : '_DRNKWEK',\n",
    "    'FRUITJU1' : 'FTJUDA1_',\n",
    "    'FRUIT1' : 'FRUTDA1_',\n",
    "    'FVBEANS' : 'BEANDAY_',\n",
    "    'FVGREEN' : 'GRENDAY_',\n",
    "    'FVORANG' : 'ORNGDAY_',\n",
    "    'VEGETAB1' : 'VEGEDA1_',\n",
    "    'STRENGTH' : 'STRFREQ_'\n",
    "}\n",
    "\n",
    "# In features_to_keep replace the key of origin_calculated_features with the value\n",
    "for key, val in origin_calculated_features.items():\n",
    "    for i, feature in enumerate(features_to_keep):\n",
    "        if key == feature:\n",
    "            features_to_keep[i] = val\n",
    "\n",
    "# Drop duplicates\n",
    "features_to_keep = list(set(features_to_keep))\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_features_indices = []\n",
    "for feature in features_to_keep:\n",
    "    selected_features_indices.append(reduced_features.index(feature))\n",
    "\n",
    "selected_features_indices = sorted(selected_features_indices)\n",
    "\n",
    "# Create a new dataset with keeping the features that are in the selected_features_indices\n",
    "reduced_data = reduced_data[:,selected_features_indices]\n",
    "\n",
    "# Also remove the features from the reduced_features list\n",
    "reduced_features_2 = []\n",
    "for feature in reduced_features:\n",
    "    if feature in features_to_keep:\n",
    "        reduced_features_2.append(feature)\n",
    "\n",
    "# Remove redundant features\n",
    "redundant_features = [ 'FMONTH','IDATE','IMONTH','IDAY','IYEAR', 'SEQNO', '_STATE', '_PSU', ]\n",
    "# Get the indices of these features\n",
    "redundant_features_indices = []\n",
    "for feature in redundant_features:\n",
    "    redundant_features_indices.append(reduced_features_2.index(feature))\n",
    "\n",
    "# Create a new dataset with removing the features that are in the selected_features_indices\n",
    "reduced_data = np.delete(reduced_data, redundant_features_indices, 1)\n",
    "reduced_features_2 = [reduced_features_2[i] for i in range(len(reduced_features_2)) if i not in redundant_features_indices]\n",
    "\n",
    "# Replace nine values with NaNs\n",
    "replace_nine_with_nan(reduced_data)\n",
    "\n",
    "# Replace seven values with NaNs\n",
    "replace_seven_with_nan(reduced_data)\n",
    "\n",
    "# Replace 99 values with NaNs\n",
    "replace_99_with_nan(reduced_data)\n",
    "\n",
    "# For the _DRNKWEK feature, replace 9990 with NaN\n",
    "for i in range(reduced_data.shape[0]):\n",
    "    if reduced_data[i, reduced_features_2.index('_DRNKWEK')] == 9990:\n",
    "        reduced_data[i, reduced_features_2.index('_DRNKWEK')] = np.nan\n",
    "\"\"\"\n",
    "\n",
    "# Remove outliers\n",
    "reduced_data = clean_outliers_modified(reduced_data)\n",
    "\n",
    "# Replace NaNs with medians\n",
    "reduced_mean = replace_NaN(reduced_data, method='mean')\n",
    "\n",
    "# Standardize the data\n",
    "standardized_x = standardize_data(reduced_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "NB_COL = standardized_x.shape[1] # corresponds to 'D' = number of features\n",
    "NB_ROWS = standardized_x.shape[0] # corresponds to 'N' = number of observations/respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4911846554733139\n",
      "[0. 1. 1. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# GD\n",
    "w_mse_gd, loss_mse_gd = mean_squared_error_gd(y_train, standardized_x, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.001)\n",
    "print(loss_mse_gd)\n",
    "y_mse_gd = standardized_x @ w_mse_gd\n",
    "y_mse_gd = convert_predict(y_mse_gd)\n",
    "print(y_mse_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47718365272642943\n",
      "[0. 1. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Least squares\n",
    "w_ls, loss_ls = least_squares(y_train, standardized_x)\n",
    "print(loss_ls)\n",
    "y_ls = convert_predict(standardized_x @ w_ls)\n",
    "print(y_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4796769756053548\n",
      "[0. 1. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression\n",
    "w_rr, loss_rr = ridge_regression(y_train, standardized_x, lambda_ = 0.5)\n",
    "print(loss_rr)\n",
    "y_rr = convert_predict(standardized_x @ w_rr)\n",
    "print(y_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5978324012940669\n",
      "[0. 1. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression (GD)\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train, standardized_x, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.2)\n",
    "print(loss_lrgd)\n",
    "y_lrgd = convert_predict(standardized_x @ w_lrgd)\n",
    "#y_lrgd = convert_0_to_minus1(y_sigmoid)\n",
    "print(y_lrgd)\n",
    "#log sgd\n",
    "#w_lrsgd, loss_lrsgd = logistic_regression_SGD(y_train, x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6261911346748003\n",
      "[0. 1. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "w_reg, loss_reg = reg_logistic_regression(y_train, standardized_x, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n",
    "print(loss_reg)\n",
    "y_reg = convert_predict(standardized_x @ w_reg)\n",
    "print(y_reg)\n",
    "# reg log sgd\n",
    "#w_reg_lrsgd, loss_reg_lrsgd = reg_logistic_regression_SGD(y_train, x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.1, lambda_ = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD VERSION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take care of the NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nans = np.zeros(nb_col)\n",
    "for i, col in enumerate(x_train.T):\n",
    "    nb_nans[i] = np.count_nonzero(np.isnan(col))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(nb_nans, bins=20)\n",
    "plt.title(\"Number of NaNs per feature before\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_x_train, reduced_ids = removing_features(nb_nans, train_ids, x_train)\n",
    "\n",
    "for i, col in enumerate(reduced_x_train.T):\n",
    "    nb_nans[i] = np.count_nonzero(np.isnan(col))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(nb_nans, bins=20)\n",
    "plt.title(\"Number of NaNs per feature after\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize and remove nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the columns that have the same value in it (avoid std=0)\n",
    "reduced_x_train, reduced_ids = remove_identic_col(reduced_ids, reduced_x_train)\n",
    "nb_col_reduced = reduced_x_train.shape[1]\n",
    "print(nb_col_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standardized = standardize_data(reduced_x_train)\n",
    "\n",
    "clean_x_train = replace_NaN(x_standardized, method = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the methods a first time to see how we're doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "NB_COL = clean_x_train.shape[1] # corresponds to 'D' = number of features\n",
    "NB_ROWS = clean_x_train.shape[0] # corresponds to 'N' = number of observations/respondents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse_gd, loss_mse_gd = mean_squared_error_gd(y_train, clean_x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.001)\n",
    "print(loss_mse_gd)\n",
    "y_mse_gd = clean_x_train @ w_mse_gd\n",
    "y_mse_gd = convert_predict(y_mse_gd)\n",
    "print(y_mse_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse_sgd, loss_mse_sgd = mean_squared_error_sgd(y_train, clean_x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.001)\n",
    "print(loss_mse_gd)\n",
    "y_mse_gd = clean_x_train @ w_mse_gd\n",
    "y_mse_gd = convert_predict(y_mse_gd)\n",
    "print(y_mse_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "w_rr, loss_rr = ridge_regression(y_train, x_train, lambda_ = 0.5)\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train, x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n",
    "w_lrsgd, loss_lrsgd = logistic_regression_SGD(y_train, x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n",
    "w_reg_lrgd, loss_reg_lrgd = reg_logistic_regression(y_train, x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n",
    "w_reg_lrsgd, loss_reg_lrsgd = reg_logistic_regression_SGD(y_train, x_train, initial_w = np.zeros(NB_COL), max_iters = 50, gamma = 0.1, lambda_ = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea to remove columns: compare means in population y=0 and y=1, and remove col for which those means are close\n",
    "(this is done already a little with remove_identic_col for the columns that have the same value in each cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_outlier_x_train = clean_outliers(reduced_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_below_10_unique_values = {}\n",
    "for i in range(reduced_x_train.shape[1]):\n",
    "    # print(f\"{reduced_features[i]}: {len(np.unique(reduced_data[:,i]))}\")\n",
    "    #print(f\"{reduced_features[i]}: {np.unique(reduced_x_train[:,i])}\")\n",
    "    if len(np.unique(reduced_x_train[:,i])) < 10:\n",
    "        features_below_10_unique_values[reduced_features[i]] = len(np.unique(reduced_x_train[:,i]))\n",
    "\n",
    "# dealing with correlation \n",
    "\n",
    "def create_dictionary_from_correlation(correlation_threshold):\n",
    "    newfeature_correlation_dict = {}\n",
    "    # For each feature in the dataset calculate the correlation with the others and save those which have higher than 0.6 correlation\n",
    "    for ft_num, feature in enumerate(reduced_features):\n",
    "        newfeature_correlation_dict[feature] = []\n",
    "        for o_ft_num, other_feature in enumerate(reduced_features):\n",
    "            if (feature != other_feature):\n",
    "                if np.abs(np.corrcoef(reduced_x_train[:,ft_num],reduced_x_train[:,o_ft_num])[0,1]) >= correlation_threshold:\n",
    "                    newfeature_correlation_dict[feature].append(other_feature)\n",
    "        print(f\" Finished for feature: {feature}\")\n",
    "    return newfeature_correlation_dict\n",
    "\n",
    "feature_correlation_dict = create_dictionary_from_correlation(0.6)\n",
    "with open('feature_correlation_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(feature_correlation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('feature_correlation_dict.pickle', 'rb') as handle:\n",
    "    feature_correlation_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the functions on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "w_mse_gd, loss_mse_gd = mean_squared_error_gd(y_train, x_train, initial_w = np.array([0,0]), max_iters = 50, gamma = 0.1)\n",
    "w_mse_sgd, loss_mse_sgd = mean_squared_error_sgd(y_train, x_train, initial_w = np.array([0,0]), max_iters = 50, gamma = 0.1)\n",
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "w_rr, loss_rr = ridge_regression(y_train, x_train, lambda_ = 0.5)\n",
    "w_lrgd, loss_lrgd = logistic_regression_GD(y_train, x_train, initial_w = np.array([0,0]), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n",
    "w_lrsgd, loss_lrsgd = logistic_regression_SGD(y_train, x_train, initial_w = np.array([0,0]), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n",
    "w_reg_lrgd, loss_reg_lrgd = regularized_logistic_regression_GD(y_train, x_train, initial_w = np.array([0,0]), max_iters = 50, gamma = 0.1, lambda_ = 0.5)\n",
    "w_reg_lrsgd, loss_reg_lrsgd = regularized_logistic_regression_SGD(y_train, x_train, initial_w = np.array([0,0]), max_iters = 50, gamma = 0.1, lambda_ = 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
