{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_given import *\n",
    "from preprocess import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train loaded\n",
      "x_train loaded\n",
      "x_test loaded\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"dataset/\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "328135\n"
     ]
    }
   ],
   "source": [
    "y_01 = convert_minus1_to_0(y_train)\n",
    "nb_col = x_train.shape[1]\n",
    "nb_rows = x_train.shape[0]\n",
    "print(nb_col)\n",
    "print(nb_rows)\n",
    "#get the features names\n",
    "with open('dataset/x_train.csv', 'r') as f:\n",
    "    features_string = f.readline()\n",
    "    features = features_string.split(',')\n",
    "features = features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ids = np.arange(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nan = nb_of_nans(x_train)\n",
    "# Get reduced data\n",
    "reduced_data, reduced_features, removed_features = removing_features(nb_nan, features, x_train)\n",
    "# remove the same columns from x_test\n",
    "reduced_x_test = np.delete(x_test, removed_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_ids = np.delete(features_ids, removed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 139)\n"
     ]
    }
   ],
   "source": [
    "print(reduced_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139,)\n"
     ]
    }
   ],
   "source": [
    "print(reduced_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_x_1 = x_train[:, reduced_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the right preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature correlation dictionary\n",
    "feature_correlation_dict = create_dictionary_from_correlation(reduced_data,reduced_features,0.6)\n",
    "\n",
    "max_corrr_feature_dict = {}\n",
    "\n",
    "\n",
    "for key, val in feature_correlation_dict.items():\n",
    "    max_corrr_feature_dict[key] = len(val)\n",
    "# Sort the dictionary by value in descending order\n",
    "max_corrr_feature_dict = {k: v for k, v in sorted(max_corrr_feature_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "features_to_drop = []\n",
    "for key in max_corrr_feature_dict.keys():\n",
    "    features_to_drop.append(key)\n",
    "    if len(features_to_drop) == 30:\n",
    "        break\n",
    "\n",
    "# Define features to keep\n",
    "features_to_keep = []\n",
    "for feature in reduced_features:\n",
    "    if feature not in features_to_drop:\n",
    "        features_to_keep.append(feature)\n",
    "\n",
    "# Also replace some features with their calculated counterparts\n",
    "origin_calculated_features = {\n",
    "    'WEIGHT2' : 'WTKG3',\n",
    "    'HEIGHT3' : 'HTM4',\n",
    "    'ALCDAY5' : '_DRNKWEK',\n",
    "    'FRUITJU1' : 'FTJUDA1_',\n",
    "    'FRUIT1' : 'FRUTDA1_',\n",
    "    'FVBEANS' : 'BEANDAY_',\n",
    "    'FVGREEN' : 'GRENDAY_',\n",
    "    'FVORANG' : 'ORNGDAY_',\n",
    "    'VEGETAB1' : 'VEGEDA1_',\n",
    "    'STRENGTH' : 'STRFREQ_'\n",
    "}\n",
    "\n",
    "# In features_to_keep replace the key of origin_calculated_features with the value\n",
    "for key, val in origin_calculated_features.items():\n",
    "    for i, feature in enumerate(features_to_keep):\n",
    "        if key == feature:\n",
    "            features_to_keep[i] = val\n",
    "\n",
    "# Drop duplicates\n",
    "features_to_keep = list(set(features_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_features_indices = []\n",
    "selected_features_indices_original = []\n",
    "for feature in features_to_keep:\n",
    "    selected_features_indices.append(reduced_features.index(feature))\n",
    "    selected_features_indices_original.append(features.index(feature))\n",
    "\n",
    "selected_features_indices = sorted(selected_features_indices)\n",
    "selected_features_indices_original = sorted(selected_features_indices_original)\n",
    "\n",
    "# Create a new dataset with keeping the features that are in the selected_features_indices\n",
    "reduced_data = reduced_data[:,selected_features_indices]\n",
    "reduced_x_test = reduced_x_test[:,selected_features_indices]\n",
    "\n",
    "# Also remove the features from the reduced_features list\n",
    "reduced_features_2 = []\n",
    "for feature in reduced_features:\n",
    "    if feature in features_to_keep:\n",
    "        reduced_features_2.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove redundant features\n",
    "redundant_features = [ 'FMONTH','IDATE','IMONTH','IDAY','IYEAR', 'SEQNO', '_STATE', '_PSU', ]\n",
    "# Get the indices of these features\n",
    "redundant_features_indices = []\n",
    "redundant_features_indices_original = []\n",
    "for feature in redundant_features:\n",
    "    redundant_features_indices.append(reduced_features_2.index(feature))\n",
    "    redundant_features_indices_original.append(features.index(feature))\n",
    "\n",
    "# Create a new dataset with removing the features that are in the selected_features_indices\n",
    "reduced_data = np.delete(reduced_data, redundant_features_indices, 1)\n",
    "reduced_features_2 = [reduced_features_2[i] for i in range(len(reduced_features_2)) if i not in redundant_features_indices]\n",
    "reduced_x_test = np.delete(reduced_x_test, redundant_features_indices, 1)\n",
    "\n",
    "# Replace nine values with NaNs\n",
    "replace_nine_with_nan(reduced_data)\n",
    "replace_nine_with_nan(reduced_x_test)\n",
    "\n",
    "# Replace seven values with NaNs\n",
    "replace_seven_with_nan(reduced_data)\n",
    "replace_seven_with_nan(reduced_x_test)\n",
    "\n",
    "# Replace 99 values with NaNs\n",
    "replace_99_with_nan(reduced_data)\n",
    "replace_99_with_nan(reduced_x_test)\n",
    "\n",
    "# For the _DRNKWEK feature, replace 9990 with NaN\n",
    "for i in range(reduced_data.shape[0]):\n",
    "    if reduced_data[i, reduced_features_2.index('_DRNKWEK')] == 9990:\n",
    "        reduced_data[i, reduced_features_2.index('_DRNKWEK')] = np.nan\n",
    "for i in range(reduced_x_test.shape[0]):\n",
    "    if reduced_x_test[i, reduced_features_2.index('_DRNKWEK')] == 9990:\n",
    "        reduced_x_test[i, reduced_features_2.index('_DRNKWEK')] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_ids_2 = [i for i in selected_features_indices_original if i not in redundant_features_indices_original]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "print(reduced_data.shape[1])\n",
    "print(len(reduced_ids_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_x = x_train[:, reduced_ids_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.,  1.,  5., ...,  3.,  4.,  2.],\n",
       "        [ 4., 88., 88., ...,  3.,  4., nan],\n",
       "        [ 2., 77., 77., ...,  2.,  3.,  2.],\n",
       "        ...,\n",
       "        [ 3., 88.,  1., ...,  3.,  4.,  2.],\n",
       "        [ 3., 88., 88., ...,  3.,  4.,  2.],\n",
       "        [ 2.,  7.,  7., ...,  2.,  3.,  2.]]),\n",
       " array([[ 2.,  1.,  5., ...,  3.,  4.,  2.],\n",
       "        [ 4.,  0.,  0., ...,  3.,  4., nan],\n",
       "        [ 2., nan, nan, ...,  2.,  3.,  2.],\n",
       "        ...,\n",
       "        [ 3.,  0.,  1., ...,  3.,  4.,  2.],\n",
       "        [ 3.,  0.,  0., ...,  3.,  4.,  2.],\n",
       "        [ 2.,  7.,  7., ...,  2.,  3.,  2.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_x, reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nine_with_nan(check_x)\n",
    "replace_seven_with_nan(check_x)\n",
    "replace_99_with_nan(check_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(check_x.shape[0]):\n",
    "    if check_x[i, reduced_features_2.index('_DRNKWEK')] == 9990:\n",
    "        check_x[i, reduced_features_2.index('_DRNKWEK')] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098153\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(check_x!=reduced_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "reduced_data = clean_outliers_modified(reduced_data)\n",
    "reduced_test = clean_outliers_modified(reduced_x_test)\n",
    "\n",
    "# Replace NaNs with medians\n",
    "reduced_median = replace_NaN(reduced_data, method='median')\n",
    "reduced_median_test = replace_NaN(reduced_test, method='median')\n",
    "\n",
    "# Standardize the data\n",
    "standardized_x = standardize_data(reduced_median)\n",
    "standardized_test = standardize_data(reduced_median_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(tx, method='mean'):\n",
    "    if method=='mean':\n",
    "        tx = np.where(np.isnan(tx), np.ma.array(tx, mask=np.isnan(tx)).mean(axis=0), tx) \n",
    "    elif method=='median':\n",
    "        tx = np.where(np.isnan(tx), np.ma.array(tx, mask=np.isnan(tx)).median(axis=0), tx) \n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(reduced_data)[np.isnan(reduced_data)==True].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_2 = reduced_data\n",
    "reduced_2[np.isnan(reduced_2)] = np.nanmedian(reduced_2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD with all preprocess\n",
    "F1 = np.zeros(5)\n",
    "accuracy = np.zeros(5)\n",
    "losses = np.zeros(5)\n",
    "for degree in range(5):\n",
    "    tx = build_poly(standardized_x, degree)\n",
    "    w0 = np.zeros(tx.shape[1])\n",
    "    w_mse_gd, loss_mse_gd = mean_squared_error_gd(y_01, tx, w0, max_iters = 50, gamma = 0.01)\n",
    "    y_mse_gd = convert_predict(tx @ w_mse_gd)\n",
    "    F1[degree] = compute_f1(y_01, y_mse_gd)\n",
    "    accuracy[degree] = compute_accuracy(y_01, y_mse_gd)\n",
    "    losses[degree] = loss_mse_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(F1)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_means = np.nanmean(x_train, axis = 0)\n",
    "tx = x_train\n",
    "tx_clean = np.where(np.isnan(tx), np.ma.array(tx, mask=np.isnan(tx)).mean(axis=0), tx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tx_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tx, w):\n",
    "    return convert_predict(tx@w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = mean_squared_error_gd(y_01, tx_clean, np.zeros(tx_clean.shape[1]), max_iters=5, gamma=0.01)\n",
    "a = compute_accuracy(y_01, prediction(tx_clean, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = x_train\n",
    "replace_NaN(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = mean_squared_error_gd(y_01, tx_clean, np.zeros(tx_clean.shape[1]), max_iters=50, gamma=0.01)\n",
    "a = compute_accuracy(y_01, prediction(tx_clean, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_fold = 4\n",
    "max_iter = 50\n",
    "k_indices = build_k_indices(y_01, k_fold, seed=1)\n",
    "gammas = [0.01, 1e-2, 1e-4, 1e-5]\n",
    "\n",
    "for gamma_ in gammas:\n",
    "    accuracy = []\n",
    "    for k in range(k_fold):\n",
    "        te_indice = k_indices[k]\n",
    "        tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "        tr_indice = tr_indice.reshape(-1)\n",
    "        y_te = y_01[te_indice]\n",
    "        y_tr = y_01[tr_indice]\n",
    "        tx_te = tx_clean[te_indice]\n",
    "        tx_tr = tx_clean[tr_indice]\n",
    "        w, loss_tr = mean_squared_error_gd(y_tr, tx_tr, np.zeros(tx_tr.shape[1]), max_iter, gamma_)\n",
    "        a = compute_accuracy(y_te, prediction(tx_te, w))\n",
    "        accuracy.append(a)\n",
    "    print(\"for gamma= \", gamma_)\n",
    "    print(np.mean(accuracy), np.std(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
